{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "483f60ea-c241-47df-9d9e-3e667bd29016",
   "metadata": {},
   "source": [
    "**Project Introduction & Overview:**\n",
    "\n",
    "This project investigates model selection bias in machine learning, following the insights of Cawley and Talbot (2010), who highlighted that hyperparameter tuning on the same data used for performance evaluation can lead to overfitting and overly optimistic accuracy estimates.\n",
    "\n",
    "Using a k-NN classifier to predict breast cancer diagnoses from four predictor variables, we explore how standard grid search cross-validation can inflate performance metrics by exploiting chance patterns in the training data. \n",
    "\n",
    "To address this, we implement nested cross-validation, which separates model selection from evaluation, providing a more unbiased estimate of real-world performance. \n",
    "\n",
    "Through this process, we compare different k values and assess their true predictive power, illustrating the importance of careful validation in machine learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0902d20e-9ec9-4bcb-b04d-c1c1acac54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "188de55a-d1d5-46d6-921b-eeac9da72f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "b_data = pd.read_csv(\"breast-cancer-data.csv\")\n",
    "b_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e8a9921-5230-4eef-9173-397d15939184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 512, Validation size: 57\n"
     ]
    }
   ],
   "source": [
    "#train and split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Select predictor and target columns\n",
    "features = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean']\n",
    "target = 'diagnosis'\n",
    "\n",
    "X = b_data[features]\n",
    "y = b_data[target]\n",
    "\n",
    "#Split into train (90%) and validation (10%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Validation size: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1a799a3-b374-4159-9cc1-55ea67c530a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal k: 22\n",
      "Best cross-validation accuracy: 0.9043\n"
     ]
    }
   ],
   "source": [
    "# Part 2: Grid search for optimal k\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Define pipeline, scaling and k-NN\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "#Define parameter grid for k\n",
    "param_grid = {'knn__n_neighbors': range(1, 31)}\n",
    "\n",
    "\n",
    "#Grid search (using cross-validation on the training data only)\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Optimal k: {grid_search.best_params_['knn__n_neighbors']}\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07765e2f-0600-4c5e-9e04-46a7076b8548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6919c27f-7541-4071-b3d7-7ede97324a5f",
   "metadata": {},
   "source": [
    "**About the Paper by Cawley and Talbot:**\n",
    "\n",
    "Cawley and Talbot (2010) discuss a significant yet frequently neglected issue in machine learning: overfitting during model selection, which leads to selection bias in model evaluation.\n",
    "\n",
    "They argue that when a model's hyperparameters (for instance, the number of neighbors k in a k-NN classifier) are tuned using the same data subsequently used for estimating generalization performance, the selection process can overfit to the noise present in that dataset.\n",
    "\n",
    "When hyperparameters are optimized to maximize those estimates, the model may appear to perform exceptionally well, not due to genuine learning, but because it has adapted to idiosyncrasies of the dataset used in tuning.\n",
    "\n",
    "Because of this, the model's reported accuracy is optimistically biased, which means that it will usually do worse on unseen data than it did during cross-validation.\n",
    "The paper emphasizes that this problem is especially bad when the same dataset is used for both model selection and final performance reporting. This lets information leak from the training phase to the evaluation phase.\n",
    "\n",
    "Cawley & Talbot write that “model selection should be viewed as an integral part of model fitting” and warn that common evaluation practices “are susceptible to a form of selection bias” (p. 2079–2080, 2102–2105).\n",
    "\n",
    "**How this applies specifically to our scenario:**\n",
    "\n",
    "We utilized a k-NN classifier to predict breast cancer diagnosis based on four predictor variables (radius_mean, texture_mean, perimeter_mean, and area_mean).\n",
    "\n",
    "To find the best model, we conducted a grid search over different values of k, using cross-validation on the same 90% training data.\n",
    "However, as Cawley and Talbot describe, this procedure introduces model selection bias:\n",
    "\n",
    "- The grid search optimizes k to maximize CV accuracy on the training data.\n",
    "- Because the CV process is not perfectly reliable (it has variance), the chosen k may exploit chance patterns or noise in the training folds.\n",
    "- When we later evaluate this tuned model, its performance may appear inflated compared to its true generalization ability.\n",
    "\n",
    "This mirrors the major issue described in the paper. Our grid search has effectively overfit the model selection criterion, making our evaluation over-optimistic.\n",
    "\n",
    "To mitigate this, we have to separate model selection from performance evaluation. For example, through nested cross-validation or by holding out an independent validation set, the 10% we reserved earlier.\n",
    "\n",
    "This ensures that the data used to select hyperparameters is distinct from the data used to evaluate them, producing an unbiased estimate of real-world performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff765f-e69c-4c80-8c52-0ec190c357cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd7b2f85-e691-4b31-ab1b-3fa1264ddfbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Previous Approach (Single Grid Search CV):\n",
      "Best k: 22\n",
      "Training CV Accuracy: 0.9043\n",
      "Validation Accuracy (left-out 10% set): 0.9298\n",
      "\n",
      "New Approach (Nested Cross-Validation):\n",
      "Nested CV mean accuracy: 0.9061\n",
      "Nested CV standard deviation:0.0377\n",
      "Best k (nested): 27\n",
      "Validation Accuracy (left-out 10% set): 0.9298\n"
     ]
    }
   ],
   "source": [
    "#Part 5: Implementing an unbiased model selection approach\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "#Recall pipe, param_grid, X_train, y_train, and X_val, y_val from previous parts\n",
    "\n",
    "#previous  performance report\n",
    "best_k_old = grid_search.best_params_['knn__n_neighbors']\n",
    "train_acc_old = grid_search.best_score_\n",
    "val_acc_old = grid_search.score(X_val, y_val)\n",
    "\n",
    "print(\"\\nPrevious Approach (Single Grid Search CV):\")\n",
    "print(f\"Best k: {best_k_old}\")\n",
    "print(f\"Training CV Accuracy: {train_acc_old:.4f}\")\n",
    "print(f\"Validation Accuracy (left-out 10% set): {val_acc_old:.4f}\")\n",
    "\n",
    "\n",
    "#New Approach: Nested Cross-Validation (unbiased model selection)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#Reuse the same pipeline and parameter grid\n",
    "nested_grid = GridSearchCV(pipe, param_grid, cv=inner_cv, scoring='accuracy')\n",
    "\n",
    "#Perform nested cross-validation (outer loop)\n",
    "nested_scores = cross_val_score(nested_grid, X_train, y_train, cv=outer_cv, scoring='accuracy')\n",
    "\n",
    "print(\"\\nNew Approach (Nested Cross-Validation):\")\n",
    "print(f\"Nested CV mean accuracy: {nested_scores.mean():.4f}\")\n",
    "print(f\"Nested CV standard deviation:{nested_scores.std():.4f}\")\n",
    "\n",
    "#Fit final model using full training data (with inner grid search)\n",
    "nested_grid.fit(X_train, y_train)\n",
    "best_k_new = nested_grid.best_params_['knn__n_neighbors']\n",
    "val_acc_new = nested_grid.score(X_val, y_val)\n",
    "\n",
    "print(f\"Best k (nested): {best_k_new}\")\n",
    "print(f\"Validation Accuracy (left-out 10% set): {val_acc_new:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3108a2b5-5d88-4789-9d78-a81a9756a153",
   "metadata": {},
   "source": [
    "**Comparison:**\n",
    "\n",
    "- The single grid search CV selected k = 22 with a training accuracy of 0.9043 and validation accuracy of 0.9298. \n",
    "\n",
    "- The nested cross-validation found k = 27, with a mean accuracy of 0.9061 with a variability of about 3.77% depending on which subset of data it was\n",
    "  trained and tested on, and the same validation accuracy. \n",
    "\n",
    "While both performed similarly, the nested CV gives a more reliable, unbiased estimate since it separates model tuning from evaluation. \n",
    "\n",
    "This aligns with Cawley & Talbot (2010), who highlight that independent validation prevents selection bias and overfitting in model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06574be3-9eff-499a-b55c-f6c29fba0936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
